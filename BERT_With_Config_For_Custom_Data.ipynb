{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_With_Config_For_Custom_Data",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhsNXZOKoU7_",
        "outputId": "53d93861-1cf4-43d5-be33-9522568bee58"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuCQeoCvoaTR"
      },
      "source": [
        "# Install transformers.\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmH7ujpyoav0"
      },
      "source": [
        "# Imports.\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    BertConfig,\n",
        "    BertForMaskedLM,\n",
        "    LineByLineTextDataset,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EvalPrediction\n",
        ")\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSxmvp4hzIaX"
      },
      "source": [
        "# Requirements\n",
        "* All sequences must be same length.\n",
        "* Read sequences from a repo for example Google Storage or Google Drive.\n",
        "* Divide data into 2. One for training one for validation.\n",
        "* Read features in order to create tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_eBCprqomhs"
      },
      "source": [
        "# Use rb for byte data, use r for string data.\n",
        "with open(\"\", \"rb\") as f:\n",
        "    columns = pickle.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHOq3oMXooJS"
      },
      "source": [
        "data_mapper = {}\n",
        "\n",
        "# Change this according to your requirements.\n",
        "for idx, col in enumerate(columns):\n",
        "    data_mapper[int(col)] = int(idx + 1)\n",
        "\n",
        "data_mapper[0] = 0\n",
        "data_mapper[302] = len(data_mapper)\n",
        "data_mapper[303] = len(data_mapper)\n",
        "data_mapper[304] = len(data_mapper)\n",
        "data_mapper[305] = len(data_mapper)\n",
        "data_mapper[306] = len(data_mapper)\n",
        "data_mapper[307] = len(data_mapper)\n",
        "\n",
        "inverse_data_mapper = {value: key for key, value in data_mapper.items()}\n",
        "\n",
        "\n",
        "class Tokenizer():\n",
        "    \"\"\"Tokenizer class for reading and parsing input data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dict, inverse_input_dict, str_dict):\n",
        "        \"\"\"Init func.\n",
        "        \n",
        "            input_dict: A dictionary holds event_ids as keys and indexes as values.\n",
        "            inverse_input_dict: Reverse dictionary of the input_dict.\n",
        "            str_dict: event ids as keys str' s as values.\n",
        "        \"\"\"\n",
        "        self.dictionary = input_dict\n",
        "        self.inverse_dictionary = inverse_input_dict\n",
        "        self.str_dict = str_dict\n",
        "        self.cls_token_ids = self.dictionary[304]\n",
        "        self.sep_token_id = self.dictionary[305]\n",
        "        self.pad_token_id = self.dictionary[0]\n",
        "        self.mask_token_id = self.dictionary[306]\n",
        "        self.unk_token_id = self.dictionary[303]\n",
        "        self.padding_side = \"left\"\n",
        "        self.model_input_names = [\"token_type_ids\"]\n",
        "\n",
        "        # Change these tokens according to yyour needs.\n",
        "        self.mask_token = 306\n",
        "        self.sep_token = 305\n",
        "        self.cls_token = 304\n",
        "        self.unk_token = 303\n",
        "        self.pad_token = 302\n",
        "\n",
        "    def convert_tokens_to_ids(self, input_sequence):\n",
        "        \"\"\"Tokens to ids.\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for seq in input_sequence:\n",
        "            result.append(self.dictionary[seq])\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def convert_ids_to_tokens(self, input_ids):\n",
        "        \"\"\"Ids to tokens.\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for inp in input_ids:\n",
        "            result.append(self.inverse_dictionary[inp])\n",
        "        \n",
        "        return result\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Tokens to string.\n",
        "        \"\"\"\n",
        "        return_sequence = []\n",
        "        for tok in tokens:\n",
        "            return_sequence.append(self.str_dict[tok])\n",
        "        \n",
        "        return return_sequence\n",
        "\n",
        "    def __call__(self, sequences, add_special_tokens=True, truncation=True, max_length=64):\n",
        "        \"\"\"This function called when a batch passed to the tokenizer.\n",
        "            \n",
        "            sequences: List of sequences. (Iterable.)\n",
        "            add_special_tokens: Are special tokens added to the sequences.\n",
        "            truncation: Longer sequences will be truncated.\n",
        "                This is not implemented because all the sequences padded.\n",
        "            max_length: Batch_size.\n",
        "        \"\"\"\n",
        "        return_dict = {\n",
        "            \"input_ids\": []\n",
        "        }\n",
        "        for sequence in sequences:\n",
        "            return_dict[\"input_ids\"].append(self.build_inputs_with_special_tokens(self.__convert_to_int_sequence(sequence)))\n",
        "\n",
        "        return return_dict\n",
        "\n",
        "    def __convert_to_int_sequence(self, sequence):\n",
        "        new_seq = sequence.split(\" \")\n",
        "        return list(map(int, new_seq))\n",
        "\n",
        "    def tokenize(self, input_seq):\n",
        "        return input_seq\n",
        "\n",
        "    def mask_last_element(self, sequence):\n",
        "        sequence[-2] = self.mask_token_id\n",
        "        return sequence\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.dictionary)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.dictionary\n",
        "    \n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0, token_ids_1 = None\n",
        "    ):\n",
        "    \n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_ids] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_ids]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "# Create tokenizer object.\n",
        "tokenizer = Tokenizer(data_mapper, inverse_data_mapper)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0ENDV0ToxeM",
        "outputId": "0c4fa0ac-8ed3-4ba9-bdf6-b5aee705089f"
      },
      "source": [
        "# Read dataset.\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTPGc_gnozBI",
        "outputId": "3f9f55bf-58d6-4287-a5b0-b93284a3ca8f"
      },
      "source": [
        "# Read dataset.\n",
        "eval_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLUyaLWeo0g2"
      },
      "source": [
        "class DataCollator():\n",
        "    \"\"\"Process input on the fly.\n",
        "    \"\"\"\n",
        "    def __call__(self, examples):\n",
        "        \"\"\"Called when a parameter passed to the object.\n",
        "        \"\"\"\n",
        "        batch = {\"input_ids\": self._collate_batch(examples, tokenizer)}\n",
        "\n",
        "        batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n",
        "            batch[\"input_ids\"],\n",
        "        )\n",
        "        return batch\n",
        "    \n",
        "    def _collate_batch(self, examples, tokenizer):\n",
        "        \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
        "        # Tensorize if necessary.\n",
        "        if isinstance(examples[0][\"input_ids\"], (list, tuple)):\n",
        "            examples = [torch.tensor(e[\"input_ids\"], dtype=torch.long) for e in examples]\n",
        "\n",
        "        # Check if padding is necessary.\n",
        "        length_of_first = examples[0][\"input_ids\"].size(0)\n",
        "        are_tensors_same_length = all(x[\"input_ids\"].size(0) == length_of_first for x in examples)\n",
        "        if are_tensors_same_length:\n",
        "            examples = [example[\"input_ids\"] for example in examples]\n",
        "            return torch.stack(examples, dim=0)\n",
        "    \n",
        "    def mask_tokens(\n",
        "        self, inputs: torch.Tensor, special_tokens_mask = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "        \"\"\"\n",
        "        labels = inputs.clone()\n",
        "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
        "        probability_matrix = torch.full(labels.shape, .15)\n",
        "\n",
        "        if special_tokens_mask is None:\n",
        "            # special_tokens_mask = self.get_special_tokens_mask(labels, already_has_special_tokens=True)\n",
        "            # special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "            special_tokens_mask = [\n",
        "                self.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "            ]\n",
        "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "        else:\n",
        "            special_tokens_mask = special_tokens_mask.bool()\n",
        "        \n",
        "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "        inputs[indices_replaced] = torch.tensor(tokenizer.convert_tokens_to_ids([306]))\n",
        "\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(tokenizer.dictionary), labels.shape, dtype=torch.long)\n",
        "        inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "        return inputs, labels\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self, token_ids_0, token_ids_1 = None, already_has_special_tokensl = False\n",
        "    ):\n",
        "\n",
        "        all_special_ids = [133, 129, 130, 131, 132, 0]\n",
        "        special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]\n",
        "\n",
        "        return special_tokens_mask\n",
        "\n",
        "data_collator = DataCollator()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH1iuu0Xo4VG"
      },
      "source": [
        "vocabulary_size = len(data_mapper)\n",
        "seq_len = 42\n",
        "\n",
        "# Create config.\n",
        "config = BertConfig(\n",
        "    vocab_size=308,\n",
        "    hidden_size=720,\n",
        "    num_hidden_layers=12,\n",
        "    num_attention_heads=12,\n",
        "    intermediate_size=1024,\n",
        "    hidden_act=\"gelu\",\n",
        "    hidden_dropout_prob=.1,\n",
        "    attention_probs_dropout_prob=.1,\n",
        "    max_position_embeddings=seq_len,\n",
        "    initializer_range=.02,\n",
        "    layer_norm_eps=1e-12,\n",
        "    gradient_checkpointing=False,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True,\n",
        ")\n",
        "\n",
        "# Create model.\n",
        "model = BertForMaskedLM(config)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PgsObwKo4r8"
      },
      "source": [
        "def make_metrics(pred: EvalPrediction):\n",
        "    return_dict = {}\n",
        "    return_dict[\"accuracy\"] = accuracy_score(pred.label_ids, pred.predictions)\n",
        "    \n",
        "    return return_dict\n",
        "\n",
        "#Â Trainer arguments for saving, number of epochs...\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=128,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Create a trainer.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=make_metrics,\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iHEA3BwqzC8"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}